{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7eebd777-c3d6-4229-ade7-6f7b28e17007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import networkx as nx\n",
    "import random\n",
    "import time\n",
    "import multiprocessing as mp\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from itertools import repeat\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "random.seed(282)  \n",
    "np.random.seed(282) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91e01f56-e5bb-4b78-b818-8728ed99bfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUMBER_OF_TRIAL = 10\n",
    "FINITE_INFINITY = 999999\n",
    "\n",
    "def load_obj(file_addr):\n",
    "    with open(file_addr+ '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_obj(obj, file_addr ):\n",
    "    with open(file_addr + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def randomize_graph(graph, randomization_type, allow_self_edges=False):\n",
    "    \n",
    "\n",
    "    debug = False\n",
    "\n",
    "    n_node = graph.number_of_nodes()\n",
    "    n_edge = graph.number_of_edges()\n",
    "\n",
    "    if randomization_type == \"same_degree_sequence\":\n",
    "        # Takes ages to find a suitable conformation for large graphs\n",
    "        sequence = graph.degree().values()\n",
    "        new_graph = None\n",
    "        while new_graph is None:\n",
    "            new_graph = nx.random_degree_sequence_graph(sequence)\n",
    "        return new_graph\n",
    "\n",
    "    if randomization_type == \"graph_tool_correlated\":\n",
    "        try:\n",
    "            import graph_tool\n",
    "        except:\n",
    "            raise ValueError(\"Graph tool package not installed\")\n",
    "            return\n",
    "        new_graph = graph.copy()\n",
    "        graph_tool.generation.random_rewire(new_graph, model='uncorrelated', n_iter=1, edge_sweep=True,\n",
    "                                            parallel_edges=False, self_loops=False, vertex_corr=None,\n",
    "                                            block_membership=None, alias=True, cache_probs=True, persist=False,\n",
    "                                            ret_fail=False, verbose=False)\n",
    "        return new_graph\n",
    "\n",
    "    if randomization_type == \"erdos_renyi\":\n",
    "        # raise Exception(\"Work in progress\")\n",
    "        p = float(2 * n_edge) / (n_node * n_node - 2 * n_node)\n",
    "        # Chooses each of the possible [n(n-1)]/2 edges with probability p\n",
    "        new_graph = nx.erdos_renyi_graph(n_node, p)\n",
    "        mapping = dict(zip(new_graph.nodes(), graph.nodes()))\n",
    "        new_graph = nx.relabel_nodes(new_graph, mapping)\n",
    "        available_edges = graph.edges()\n",
    "\n",
    "        # Map graph from random model to new graph\n",
    "        for edge in new_graph.edges():\n",
    "            if len(available_edges) > 0:\n",
    "                edge_org = available_edges.pop()\n",
    "                if debug:\n",
    "                    print (\"From random:\", (edge[0], edge[1]))\n",
    "                new_graph.add_edge(edge[0], edge[1], graph.get_edge_data(edge_org[0], edge_org[1]))\n",
    "            # If the random model added too many edges\n",
    "            else:\n",
    "                if debug:\n",
    "                    print (\"Removing:\", edge)\n",
    "                new_graph.remove_edge(edge[0], edge[1])\n",
    "\n",
    "        # If the random model failed to add enough edges\n",
    "        nodes = new_graph.nodes()\n",
    "        for edge_org in available_edges:\n",
    "            source_id = random.choice(nodes)\n",
    "            target_id = random.choice(nodes)\n",
    "            while new_graph.has_edge(source_id, target_id) or (not allow_self_edges and source_id == target_id):\n",
    "                source_id = random.choice(nodes)\n",
    "                target_id = random.choice(nodes)\n",
    "            if debug:\n",
    "                print (\"Adding:\", (source_id, target_id))\n",
    "            new_graph.add_edge(source_id, target_id, graph.get_edge_data(edge_org[0], edge_org[1]))\n",
    "        return new_graph\n",
    "\n",
    "    if randomization_type == \"barabasi_albert\":\n",
    "        # raise Exception(\"Work in progress\")\n",
    "        if n_edge >= n_node:\n",
    "            # A graph of n nodes is grown by attaching new nodes each with m edges that are preferentially attached to existing nodes with high degree\n",
    "            new_graph = nx.barabasi_albert_graph(n_node, n_edge / n_node)\n",
    "            mapping = dict(zip(new_graph.nodes(), graph.nodes()))\n",
    "            new_graph = nx.relabel_nodes(new_graph, mapping)\n",
    "        else:\n",
    "            new_graph = nx.create_empty_copy(graph)\n",
    "\n",
    "        available_edges = graph.edges()\n",
    "        degree_map = dict(nx.degree(new_graph))\n",
    "        nodes = new_graph.nodes()\n",
    "\n",
    "        # Map graph from random model to new graph\n",
    "        for edge in new_graph.edges():\n",
    "            if len(available_edges) > 0:\n",
    "                edge_org = available_edges.pop()\n",
    "                if debug:\n",
    "                    print (\"From random:\", (edge[0], edge[1]))\n",
    "                new_graph.add_edge(edge[0], edge[1], graph.get_edge_data(edge_org[0], edge_org[1]))\n",
    "            # If the random model added too many edges\n",
    "            else:\n",
    "                nodes_to_select = [id for id, d in degree_map.items() for j in range(d + 1)]\n",
    "                source_id = random.choice(nodes())\n",
    "                target_id = random.choice(nodes_to_select)\n",
    "                if debug:\n",
    "                    print (\"Removing:\", (source_id, target_id))\n",
    "                new_graph.remove_edge(source_id, target_id)\n",
    "                degree_map[source_id] -= 1\n",
    "                degree_map[target_id] -= 1\n",
    "\n",
    "            # If the random model failed to add enough edges\n",
    "        for edge_org in available_edges:\n",
    "            nodes_to_select = [id for id, d in degree_map.items() for j in range(d + 1)]\n",
    "            source_id = random.choice(nodes)\n",
    "            target_id = random.choice(nodes_to_select)\n",
    "            while new_graph.has_edge(source_id, target_id) or (not allow_self_edges and source_id == target_id):\n",
    "                source_id = random.choice(nodes)\n",
    "                target_id = random.choice(nodes_to_select)\n",
    "            if debug:\n",
    "                print (\"Adding:\", (source_id, target_id))\n",
    "            new_graph.add_edge(source_id, target_id, graph.get_edge_data(edge_org[0], edge_org[1]))\n",
    "            degree_map[source_id] += 1\n",
    "            degree_map[target_id] += 1\n",
    "\n",
    "        return new_graph\n",
    "\n",
    "    new_graph = nx.create_empty_copy(graph)\n",
    "    # new_graph.add_nodes_from(graph.nodes())\n",
    "\n",
    "    if randomization_type == \"random\":\n",
    "        nodes = new_graph.nodes()\n",
    "        for edge in graph.edges():\n",
    "            source_id = random.choice(nodes)\n",
    "            target_id = random.choice(nodes)\n",
    "            while new_graph.has_edge(source_id, target_id) or (not allow_self_edges and source_id == target_id):\n",
    "                source_id = random.choice(nodes)\n",
    "                target_id = random.choice(nodes)\n",
    "            new_graph.add_edge(source_id, target_id, graph.get_edge_data(edge[0], edge[1]))\n",
    "\n",
    "    elif randomization_type == \"preserve_topology\":  # shuffle_nodes\n",
    "        nodes = list(graph.nodes())\n",
    "        random_nodes = list(graph.nodes())\n",
    "        random.shuffle(random_nodes)\n",
    "        equivalences = dict([(nodes[i], random_nodes[i]) for i in range(len(nodes))])\n",
    "        new_graph.add_edges_from([(equivalences[current_edge[0]], equivalences[current_edge[1]],\n",
    "                                   graph.get_edge_data(current_edge[0], current_edge[1])) for current_edge in\n",
    "                                  graph.edges()])\n",
    "\n",
    "    elif randomization_type == \"preserve_topology_and_node_degree\":  # shuffle_nodes_within_same_degree\n",
    "        nodes_by_degree = dict((degree, []) for u, degree in graph.degree())  # .values()\n",
    "        graph_degree = dict(graph.degree())\n",
    "        [nodes_by_degree[graph_degree[node]].append(node) for node in graph_degree]\n",
    "        equivalences = {}\n",
    "        for current_degree in nodes_by_degree.keys():\n",
    "            nodes = nodes_by_degree[current_degree]\n",
    "            random_nodes = list(nodes)\n",
    "            random.shuffle(random_nodes)\n",
    "            equivalences.update(dict([(nodes[i], random_nodes[i]) for i in range(len(nodes))]))\n",
    "        new_graph.add_edges_from([(equivalences[current_edge[0]], equivalences[current_edge[1]],\n",
    "                                   graph.get_edge_data(current_edge[0], current_edge[1])) for current_edge in\n",
    "                                  graph.edges()])\n",
    "\n",
    "    elif randomization_type == \"preserve_degree_distribution\":\n",
    "        for current_node1, current_node2 in graph.edges():\n",
    "            new_graph.add_edge(current_node1, current_node2, *graph.get_edge_data(current_node1, current_node2))\n",
    "        # max_degree = sorted(zip(*list(graph.degree()))[1])[-1]  # .values()\n",
    "        degree_sequence = sorted([d for n, d in graph.degree()], reverse=True)\n",
    "        max_degree = max(degree_sequence)\n",
    "        # print \"Degree sequence\", degree_sequence\n",
    "        nodes_by_degree = dict((degree, {}) for degree in range(max_degree + 1))\n",
    "        graph_degree = dict(graph.degree())\n",
    "        [nodes_by_degree[graph_degree[node]].setdefault(node) for node in graph_degree]\n",
    "        n_perturbation = random.randint(int(2 * n_edge / 3), n_edge)  # Perturb at least 66% of the edges\n",
    "        for i in range(n_perturbation):\n",
    "            n_trial = 0\n",
    "            while True:\n",
    "                n_trial += 1\n",
    "                if n_trial > MAX_NUMBER_OF_TRIAL:\n",
    "                    if debug:\n",
    "                        print(\"Warning: Max number of trials exceeded in perturbation \", i)\n",
    "                    break\n",
    "                source_id = random.choice(list(new_graph.nodes()))\n",
    "                source_degree = new_graph.degree(source_id)\n",
    "                while source_degree < 1:\n",
    "                    source_id = random.choice(list(new_graph.nodes()))\n",
    "                    source_degree = new_graph.degree(source_id)\n",
    "                target_id = random.choice(list(new_graph.neighbors(source_id)))\n",
    "                target_degree = new_graph.degree(target_id)\n",
    "                del nodes_by_degree[source_degree][source_id]\n",
    "                nodes_by_degree[source_degree - 1].setdefault(source_id)\n",
    "                if target_id == source_id:\n",
    "                    target_degree -= 1\n",
    "                del nodes_by_degree[target_degree][target_id]\n",
    "                nodes_by_degree[target_degree - 1].setdefault(target_id)\n",
    "                ## not very important to check for cases where new_source = source (v.v. for targets)\n",
    "                new_target_id = random.choice(list(nodes_by_degree[target_degree - 1].keys()))\n",
    "                if source_id == target_id:\n",
    "                    new_source_id = new_target_id\n",
    "                else:\n",
    "                    new_source_id = random.choice(list(nodes_by_degree[source_degree - 1].keys()))\n",
    "                if debug:\n",
    "                    print(source_id, target_id, \" / \", new_source_id, new_target_id)\n",
    "                    print(source_degree, target_degree)\n",
    "                ## check if going to add an existing edge or self edge\n",
    "                if new_graph.has_edge(new_source_id, new_target_id) or (\n",
    "                        not allow_self_edges and new_source_id == new_target_id):\n",
    "                    del nodes_by_degree[target_degree - 1][target_id]\n",
    "                    nodes_by_degree[target_degree].setdefault(target_id)\n",
    "                    del nodes_by_degree[source_degree - 1][source_id]\n",
    "                    nodes_by_degree[source_degree].setdefault(source_id)\n",
    "                    continue\n",
    "                if debug:\n",
    "                    print(\"rm %s %s\" % (source_id, target_id))\n",
    "                edge_data = new_graph.get_edge_data(source_id, target_id)\n",
    "                new_graph.remove_edge(source_id, target_id)\n",
    "                if debug:\n",
    "                    print(\"add %s %s\" % (new_source_id, new_target_id))\n",
    "                new_graph.add_edge(new_source_id, new_target_id, *edge_data)\n",
    "                del nodes_by_degree[target_degree - 1][new_target_id]\n",
    "                nodes_by_degree[target_degree].setdefault(new_target_id)\n",
    "                if new_source_id == new_target_id and source_id != target_id:\n",
    "                    source_degree += 1\n",
    "                del nodes_by_degree[source_degree - 1][new_source_id]\n",
    "                nodes_by_degree[source_degree].setdefault(new_source_id)\n",
    "                break\n",
    "        randomize_graph(new_graph, \"preserve_topology\")\n",
    "\n",
    "    elif randomization_type == \"preserve_degree_distribution_and_node_degree\":\n",
    "        ## add edges as well\n",
    "        for current_node1, current_node2 in graph.edges():\n",
    "            edge_data = graph.get_edge_data(current_node1, current_node2)\n",
    "            # new_graph.add_edge(current_node1, current_node2, graph.get_edge_data(current_node1, current_node2))\n",
    "            new_graph.add_edge(current_node1, current_node2)\n",
    "\n",
    "        nodes_by_degree = dict((degree, {}) for u, degree in graph.degree())\n",
    "        graph_degree = dict(graph.degree())\n",
    "        [nodes_by_degree[graph_degree[node]].setdefault(node) for node in graph_degree]\n",
    "\n",
    "       \n",
    "\n",
    "        n_perturbation = random.randint(int(n_edge / 2), n_edge)\n",
    "        for i in range(n_perturbation):\n",
    "            # nodes =  list(new_graph.nodes())\n",
    "            source_id = random.choice(list(new_graph.nodes()))\n",
    "            # source_id = random.choice(nodes)\n",
    "            # print(\"sourc_id\", source_id)\n",
    "            source_degree = new_graph.degree(source_id)\n",
    "            ## find a node for which another node with the same degree exists\n",
    "            # available_neighbors = []\n",
    "            n_trial = 0\n",
    "            while True:  # (len(nodes_by_degree[source_degree]) < 2 or len(available_neighbors) < 1):\n",
    "                n_trial += 1\n",
    "                if n_trial > MAX_NUMBER_OF_TRIAL:\n",
    "                    if debug:\n",
    "                        print(\"Warning: Max number of trials exceeded in perturbation \", i)\n",
    "                    break\n",
    "                source_id = random.choice(list(new_graph.nodes()))\n",
    "                source_degree = new_graph.degree(source_id)\n",
    "                if len(nodes_by_degree[source_degree]) < 2:\n",
    "                    continue\n",
    "                available_neighbors = []\n",
    "                ## find a neighbor for which another node with the same degree exists\n",
    "                # for neighbor_id in new_graph.neighbors_iter(source_id):   Networkx 1.x\n",
    "                for neighbor_id in new_graph.neighbors(source_id):\n",
    "                    if source_degree == new_graph.degree(neighbor_id):\n",
    "                        if len(nodes_by_degree[new_graph.degree(neighbor_id)]) > 2:\n",
    "                            available_neighbors.append(neighbor_id)\n",
    "                    else:\n",
    "                        if len(nodes_by_degree[new_graph.degree(neighbor_id)]) > 1:\n",
    "                            available_neighbors.append(neighbor_id)\n",
    "                if len(available_neighbors) < 1:\n",
    "                    continue\n",
    "                target_id = random.choice(available_neighbors)\n",
    "                target_degree = new_graph.degree(target_id)\n",
    "                ## select a new source node with different id\n",
    "                n_trial2 = 0\n",
    "                inner_break = False\n",
    "                while True:\n",
    "                    n_trial2 += 1\n",
    "                    if n_trial2 > MAX_NUMBER_OF_TRIAL:\n",
    "                        if debug:\n",
    "                            print(\"Warning: Max number of trials exceeded in perturbation \", i)\n",
    "                        inner_break = True\n",
    "                        break\n",
    "                    new_source_id = random.choice(list(nodes_by_degree[source_degree].keys()))\n",
    "                    while new_source_id == source_id:\n",
    "                        new_source_id = random.choice(list(nodes_by_degree[source_degree].keys()))\n",
    "                    new_available_neighbors = []\n",
    "                    ## find a neighbor as new target node for which id is different from target and has an id equivalent to target\n",
    "                    for neighbor_id in new_graph.neighbors(new_source_id):\n",
    "                        if target_degree == new_graph.degree(neighbor_id):\n",
    "                            new_available_neighbors.append(neighbor_id)\n",
    "                    if len(new_available_neighbors) < 1:\n",
    "                        continue\n",
    "                    new_target_id = random.choice(list(new_available_neighbors))\n",
    "                    if len(new_available_neighbors) > 1:\n",
    "                        while new_target_id == target_id:\n",
    "                            new_target_id = random.choice(list(new_available_neighbors))\n",
    "                            # print new_available_neighbors, new_target_id\n",
    "                    else:\n",
    "                        new_target_id = new_available_neighbors[0]\n",
    "                    break\n",
    "                if inner_break:\n",
    "                    break\n",
    "                if debug:\n",
    "                    print(source_id, target_id, \" / \", new_source_id, new_target_id)\n",
    "                if source_id == new_target_id or new_source_id == target_id:\n",
    "                    continue\n",
    "                if new_graph.has_edge(source_id, new_target_id) or new_graph.has_edge(new_source_id, target_id):\n",
    "                    continue\n",
    "                if debug:\n",
    "                    print(\"rm %d %d\" % (source_id, target_id))\n",
    "                    print(\"rm %d %d\" % (new_source_id, new_target_id))\n",
    "                edge_data_1 = new_graph.get_edge_data(source_id, target_id)\n",
    "                edge_data_2 = new_graph.get_edge_data(new_source_id, new_target_id)\n",
    "                new_graph.remove_edge(source_id, target_id)\n",
    "                new_graph.remove_edge(new_source_id, new_target_id)\n",
    "                if debug:\n",
    "                    print(\"add %d %d\" % (source_id, new_target_id))\n",
    "                    print(\"add %d %d\" % (new_source_id, target_id))\n",
    "                # new_graph.add_edge(source_id, new_target_id, edge_data_1)\n",
    "                # new_graph.add_edge(new_source_id, target_id, edge_data_2)\n",
    "                new_graph.add_edge(source_id, new_target_id)\n",
    "                new_graph.add_edge(new_source_id, target_id)\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"Unknown randomization type %s\" % randomization_type)\n",
    "\n",
    "    return new_graph\n",
    "\n",
    "def centrality_dict_to_list(centriality_result_dict):\n",
    "\n",
    "    centrality_result_list = []\n",
    "    for key,value in centriality_result_dict.items():\n",
    "        centrality_result_list.append([key, value])\n",
    "\n",
    "    return centrality_result_list\n",
    "\n",
    "def analysis_centrality_RWR(input):\n",
    "\n",
    "\n",
    "    original_G, process_num = input\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "    dep_num = 164\n",
    "\n",
    "\n",
    "\n",
    "    random_G = randomize_graph(original_G, randomization_type=\"preserve_degree_distribution\")\n",
    "\n",
    "\n",
    "    random_start_genes = random.choices(list(random_G.nodes()), k=332)\n",
    "    random_target_genes = random.choices(list(random_G.nodes()), k=dep_num)\n",
    "    between_centrality_dict = nx.betweenness_centrality_subset(random_G, sources=random_start_genes,\n",
    "                                                               targets=random_target_genes)\n",
    "\n",
    "    degree_centrality_dict = nx.degree_centrality(random_G)\n",
    "    eigenvector_centrality_dict = nx.eigenvector_centrality(random_G)\n",
    "\n",
    "    random_start_genes = random.choices(list(random_G.nodes()), k=1)\n",
    "\n",
    "    random_edge_BC = nx.edge_betweenness_centrality_subset(random_G, sources=random_start_genes,\n",
    "                                                           targets=random_target_genes)\n",
    "\n",
    "    random_edge_BC_noZero = dict()\n",
    "\n",
    "    for k, v in random_edge_BC.items():\n",
    "        random_edge_BC_noZero[k] = v + 0.1\n",
    "    nx.set_edge_attributes(random_G, random_edge_BC_noZero, 'weight')\n",
    "\n",
    "    start_genes_for_PR = {random_start_genes[0] : 1}\n",
    "    rwr_dict = nx.pagerank(random_G, personalization=start_genes_for_PR)\n",
    "   \n",
    "\n",
    "    return {'eigen' : eigenvector_centrality_dict, 'between':between_centrality_dict, 'degree': degree_centrality_dict, 'rwr': rwr_dict}\n",
    "\n",
    "\n",
    "def get_permutation_result_df(permutation_result, key_method):\n",
    "   \n",
    "\n",
    "    score_list = []\n",
    "    for permutation_process in permutation_result:\n",
    "        network_analisys_score_dict = permutation_process[key_method]\n",
    "        score_list += list(network_analisys_score_dict.values())\n",
    "\n",
    "    permutation_df= pd.DataFrame(score_list, columns=[key_method])\n",
    "\n",
    "    return permutation_df\n",
    "\n",
    "\n",
    "def get_key_proteins(centrality_pvalue_addr, key_protein_addr):\n",
    "    df_centrality_pvalue = pd.read_csv(centrality_pvalue_addr, index_col=0)\n",
    "\n",
    "    \n",
    "    key_genes = df_centrality_pvalue[\n",
    "        (df_centrality_pvalue['eigen_pvalue'] < 0.05) &\n",
    "        (df_centrality_pvalue['degree_pvalue'] < 0.05) &\n",
    "        (df_centrality_pvalue['rwr_pvalue'] < 0.05)\n",
    "    ].index.tolist()\n",
    "\n",
    "    \n",
    "    with open(key_protein_addr, 'w') as f:\n",
    "        for gene in key_genes:\n",
    "            f.write(gene + '\\n')\n",
    "\n",
    "    print(f\"Found {len(key_genes)} key proteins.\")\n",
    "\n",
    "def mainp03(cores):\n",
    "\n",
    "\n",
    "    original_G = load_obj(\n",
    "        \"Result/Network/metadata_M13_All_Structure_All_Shortest_Paths_graph\")\n",
    "\n",
    "    permutation_num = list(range(1,1000))\n",
    "    graph_list = [original_G] * len(permutation_num)\n",
    "   \n",
    "\n",
    "    pool = mp.Pool(processes=cores)\n",
    "\n",
    "    print(\"Start permutation test\")\n",
    "\n",
    "    permutation_result_all = pool.map(analysis_centrality_RWR, zip(graph_list, permutation_num))\n",
    "\n",
    "    pool.close()\n",
    "\n",
    "    save_obj(permutation_result_all,\"Result/Network_analysis/metadata_M13_network_analysis_permutation\")\n",
    "    print(\"Finish permutation test\")\n",
    "\n",
    "\n",
    "    print(\"Calculate p-value\")\n",
    "\n",
    "    permutation_eigen_result_all = get_permutation_result_df(permutation_result_all, 'eigen')\n",
    "    permutation_degree_result_all = get_permutation_result_df(permutation_result_all, 'degree')\n",
    "    permutation_between_result_all = get_permutation_result_df(permutation_result_all, 'between')\n",
    "    permutation_rwr_result_all = get_permutation_result_df(permutation_result_all, 'rwr')\n",
    "    \n",
    "    centrality_addr = \"Result/Network_analysis/metadata_M13_centrality_RWR_result.csv\"\n",
    "    df_centrality_result = pd.read_csv(centrality_addr, index_col=0)\n",
    "    \n",
    "    centrality_pvalue_addr = \"Result/Network_analysis/metadata_M13_centrality_RWR_result_pvalue.csv\"\n",
    "    \n",
    "    df_final_result = pd.concat(\n",
    "        [permutation_eigen_result_all, permutation_degree_result_all, permutation_between_result_all,\n",
    "         permutation_rwr_result_all], axis=1)\n",
    "    \n",
    "    print(df_final_result)\n",
    "    print(\"compute p-value for every index\")\n",
    "\n",
    "    \n",
    "    total_rows = len(df_final_result)\n",
    "    \n",
    "    # Eigen p-value calculation\n",
    "    eigen_values = df_centrality_result['Eigen'].values[:, np.newaxis]\n",
    "    eigen_greater_counts = (df_final_result['eigen'].values > eigen_values).sum(axis=1)\n",
    "    df_centrality_result['eigen_pvalue'] = eigen_greater_counts / total_rows\n",
    "    \n",
    "    # Degree p-value calculation\n",
    "    degree_values = df_centrality_result['Degree'].values[:, np.newaxis]\n",
    "    degree_greater_counts = (df_final_result['degree'].values > degree_values).sum(axis=1)\n",
    "    df_centrality_result['degree_pvalue'] = degree_greater_counts / total_rows\n",
    "    \n",
    "    # Betweenness p-value calculation\n",
    "    between_values = df_centrality_result['Between'].values[:, np.newaxis]\n",
    "    between_greater_counts = (df_final_result['between'].values > between_values).sum(axis=1)\n",
    "    df_centrality_result['between_pvalue'] = between_greater_counts / total_rows\n",
    "    \n",
    "    # RWR p-value calculation\n",
    "    rwr_values = df_centrality_result['RWR'].values[:, np.newaxis]\n",
    "    rwr_greater_counts = (df_final_result['rwr'].values > rwr_values).sum(axis=1)\n",
    "    df_centrality_result['rwr_pvalue'] = rwr_greater_counts / total_rows\n",
    "\n",
    "    df_centrality_result.to_csv(centrality_pvalue_addr)\n",
    "    \n",
    "    ################\n",
    "    # Get Key proteins\n",
    "    ################\n",
    "    print( \"Get Key proteins, p-value 0.05\")\n",
    "    key_protein_addr = \"Result/Key_proteins/metadata_M13_key_protein.txt\"\n",
    "    \n",
    "    get_key_proteins(centrality_pvalue_addr, key_protein_addr)\n",
    "    \n",
    "    print(\"Finish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bf61a1-2c88-48c4-b709-2028529dccfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mainp03(cores = 32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
